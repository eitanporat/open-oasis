{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/open-oasis/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x73c191716a90>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports and device setup\n",
        "import torch\n",
        "from dit import DiT_models\n",
        "from vae import VAE_models\n",
        "from torchvision.io import read_video, write_video\n",
        "from utils import load_prompt, load_actions, sigmoid_beta_schedule\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "from torch import autocast\n",
        "from safetensors.torch import load_model\n",
        "import os\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA is required for this notebook\"\n",
        "device = 'cuda:0'\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# User-defined parameters (editable)\n",
        "oasis_ckpt = '/root/.cache/huggingface/hub/models--Etched--oasis-500m/snapshots/4ca7d2d811f4f0c6fd1d5719bf83f14af3446c0c/oasis500m.safetensors'\n",
        "vae_ckpt = '/root/.cache/huggingface/hub/models--Etched--oasis-500m/snapshots/4ca7d2d811f4f0c6fd1d5719bf83f14af3446c0c/vit-l-20.safetensors'\n",
        "prompt_path = 'sample_data/sample_image_0.png'\n",
        "actions_path = 'sample_data/sample_actions_0.one_hot_actions.pt'\n",
        "video_offset = None  # or set an integer\n",
        "n_prompt_frames = 1  # number of frames to condition on\n",
        "num_frames = 256  # total frames to generate\n",
        "output_path = 'video.mp4'\n",
        "fps = 20\n",
        "ddim_steps = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Oasis-500M checkpoint from /root/.cache/huggingface/hub/models--Etched--oasis-500m/snapshots/4ca7d2d811f4f0c6fd1d5719bf83f14af3446c0c/oasis500m.safetensors...\n"
          ]
        }
      ],
      "source": [
        "# Load DiT model\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "model = DiT_models['DiT-S/2'](streaming=False)\n",
        "model.half()\n",
        "\n",
        "print(f\"Loading Oasis-500M checkpoint from {oasis_ckpt}...\")\n",
        "if oasis_ckpt.endswith('.pt'):\n",
        "    ckpt = torch.load(oasis_ckpt, weights_only=True)\n",
        "    model.load_state_dict(ckpt, strict=False)\n",
        "else:\n",
        "    load_model(model, oasis_ckpt)\n",
        "model = model.to(device).eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ViT-VAE-L/20 checkpoint from /root/.cache/huggingface/hub/models--Etched--oasis-500m/snapshots/4ca7d2d811f4f0c6fd1d5719bf83f14af3446c0c/vit-l-20.safetensors...\n"
          ]
        }
      ],
      "source": [
        "# Load VAE model\n",
        "vae = VAE_models['vit-l-20-shallow-encoder']()\n",
        "vae.half()\n",
        "\n",
        "print(f\"Loading ViT-VAE-L/20 checkpoint from {vae_ckpt}...\")\n",
        "if vae_ckpt.endswith('.pt'):\n",
        "    vae_state = torch.load(vae_ckpt, weights_only=True)\n",
        "    vae.load_state_dict(vae_state)\n",
        "else:\n",
        "    load_model(vae, vae_ckpt)\n",
        "vae = vae.to(device).eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = torch.compile(model)\n",
        "vae = torch.compile(vae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare sampling and noise schedules\n",
        "max_noise_level = 1000\n",
        "noise_range = torch.linspace(-1, max_noise_level - 1, ddim_steps + 1)\n",
        "noise_abs_max = 20\n",
        "stabilization_level = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_prompt_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt is image; ignoring video_offset and n_prompt_frames\n"
          ]
        }
      ],
      "source": [
        "# Load prompt and actions, move to device\n",
        "x = load_prompt(prompt_path, video_offset=video_offset, n_prompt_frames=n_prompt_frames)\n",
        "# actions = load_actions(actions_path, action_offset=video_offset)[:, :num_frames]\n",
        "x = x.to(device).half()\n",
        "# actions = actions.to(device)\n",
        "\n",
        "# VAE encoding\n",
        "B, _, C, H, W = x.shape[0], x.shape[1], x.shape[2], x.shape[3], x.shape[4]\n",
        "scaling_factor = 0.07843137255\n",
        "x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
        "with autocast('cuda', dtype=torch.half):\n",
        "    x = vae.encode(x * 2 - 1).mean * scaling_factor\n",
        "x = rearrange(x, '(b t) (h w) c -> b t c h w', t=n_prompt_frames, h=H // vae.patch_size, w=W // vae.patch_size)\n",
        "x = x[:, :n_prompt_frames]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare alpha schedules\n",
        "betas = sigmoid_beta_schedule(max_noise_level).float().to(device)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alphas_cumprod = rearrange(alphas_cumprod, 'T -> T 1 1 1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = torch.full((B, 1), 0, dtype=torch.long, device=device)\n",
        "x_curr = x.clone()\n",
        "\n",
        "with torch.no_grad():\n",
        "    with autocast('cuda', dtype=torch.half):\n",
        "        # v = model(x_curr, t, actions[:, :1], last_only=True, last_frame=True)\n",
        "        v = model(x_curr, t, last_only=True, last_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/255 [00:00<?, ?it/s]W0610 00:15:23.839000 43700 torch/_dynamo/convert_frame.py:964] [1/8] torch._dynamo hit config.recompile_limit (8)\n",
            "W0610 00:15:23.839000 43700 torch/_dynamo/convert_frame.py:964] [1/8]    function: 'rearrange' (/open-oasis/myenv/lib/python3.11/site-packages/einops/einops.py:545)\n",
            "W0610 00:15:23.839000 43700 torch/_dynamo/convert_frame.py:964] [1/8]    last reason: 1/7: tensor 'tensor' rank mismatch. expected 2, actual 5\n",
            "W0610 00:15:23.839000 43700 torch/_dynamo/convert_frame.py:964] [1/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "W0610 00:15:23.839000 43700 torch/_dynamo/convert_frame.py:964] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "100%|██████████| 255/255 [01:52<00:00,  2.26it/s]\n"
          ]
        }
      ],
      "source": [
        "# Sampling loop\n",
        "for i in tqdm(range(n_prompt_frames, num_frames)):\n",
        "    # Initialize noise chunk\n",
        "    chunk = torch.randn((B, 1, *x.shape[-3:]), device=device, dtype=torch.float16)\n",
        "    chunk = torch.clamp(chunk, -noise_abs_max, noise_abs_max)\n",
        "    x = torch.cat([x, chunk], dim=1)\n",
        "    start_frame = max(0, i + 1 - model.max_frames)\n",
        "\n",
        "    for noise_idx in reversed(range(1, ddim_steps + 1)):\n",
        "        # Build timesteps\n",
        "        t_ctx = torch.full((B, i), stabilization_level - 1, dtype=torch.long, device=device)\n",
        "        t = torch.full((B, 1), noise_range[noise_idx], dtype=torch.long, device=device)\n",
        "        t_next = torch.full((B, 1), noise_range[noise_idx - 1], dtype=torch.long, device=device)\n",
        "        t_next = torch.where(t_next < 0, t, t_next)\n",
        "        t = torch.cat([t_ctx, t], dim=1)\n",
        "        t_next = torch.cat([t_ctx, t_next], dim=1)\n",
        "\n",
        "        # Prepare model inputs\n",
        "        x_curr = x.clone()[:, start_frame:]\n",
        "        t = t[:, start_frame:]\n",
        "        t_next = t_next[:, start_frame:]\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            if model.streaming:\n",
        "                v = model(x_curr[:, -1:], t[:, -1:], last_only=True, last_frame=False)\n",
        "            else:\n",
        "                v = model(x_curr, t, last_only=True, last_frame=False)\n",
        "            # if model.streaming:\n",
        "            #     v = model(x_curr[:, -1:], t[:, -1:], actions[:, i:i+1], last_only=True, last_frame=False)\n",
        "            # else:\n",
        "            #     v = model(x_curr, t, actions[:, :i+1], last_only=True, last_frame=False)\n",
        "\n",
        "        # DDIM noise update\n",
        "        x_start = alphas_cumprod[t].sqrt() * x_curr - (1 - alphas_cumprod[t]).sqrt() * v\n",
        "        x_noise = ((1 / alphas_cumprod[t]).sqrt() * x_curr - x_start) / (1 / alphas_cumprod[t] - 1).sqrt()\n",
        "\n",
        "        # Compute next frame\n",
        "        alpha_next = alphas_cumprod[t_next]\n",
        "        alpha_next[:, :-1] = 1\n",
        "        if noise_idx == 1:\n",
        "            alpha_next[:, -1:] = 1\n",
        "        x_pred = alpha_next.sqrt() * x_start[:, -1:] + x_noise[:, -1:] * (1 - alpha_next).sqrt()\n",
        "        x[:, -1:] = x_pred[:, -1:]\n",
        "        \n",
        "    t = torch.full((B, 1), stabilization_level - 1, dtype=torch.long, device=device)\n",
        "        \n",
        "    if model.streaming:\n",
        "        # Streaming model only needs the very last frame for the cache update\n",
        "        v = model(x[:, -1:], t, last_only=True, last_frame=True)\n",
        "    # if model.streaming:\n",
        "    #     v = model(x[:, -1:], t[:, -1:], actions[:, i:i+1], last_only=True, last_frame=True)\n",
        "    # else:\n",
        "    #     v = model(x, t, actions[:, :i+1], last_only=True, last_frame=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256, 16, 18, 32])\n"
          ]
        }
      ],
      "source": [
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/open-oasis/myenv/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation saved to video.mp4.\n"
          ]
        }
      ],
      "source": [
        "# VAE decoding and save output video\n",
        "x = rearrange(x, 'b t c h w -> (b t) (h w) c')\n",
        "with torch.no_grad():\n",
        "    x = (vae.decode(x / scaling_factor) + 1) / 2\n",
        "x = rearrange(x, '(b t) c h w -> b t h w c', t=num_frames)\n",
        "x = torch.clamp(x, 0, 1)\n",
        "x = (x * 255).byte()\n",
        "write_video(output_path, x[0].cpu(), fps=fps)\n",
        "print(f\"Generation saved to {output_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): DiT(\n",
              "    (x_embedder): PatchEmbed(\n",
              "      (proj): Conv2d(16, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (norm): Identity()\n",
              "    )\n",
              "    (t_embedder): TimestepEmbedder(\n",
              "      (mlp): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (1): SiLU()\n",
              "        (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (spatial_rotary_emb): RotaryEmbedding()\n",
              "    (temporal_rotary_emb): RotaryEmbedding()\n",
              "    (external_cond): Linear(in_features=25, out_features=1024, bias=True)\n",
              "    (blocks): ModuleList(\n",
              "      (0-15): 16 x SpatioTemporalDiTBlock(\n",
              "        (s_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
              "        (s_attn): SpatialAxialAttention(\n",
              "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "        )\n",
              "        (s_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
              "        (s_mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (s_adaLN_modulation): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
              "        )\n",
              "        (t_norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
              "        (t_attn): TemporalAxialAttention(\n",
              "          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (to_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "        )\n",
              "        (t_norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
              "        (t_mlp): Mlp(\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (act): GELU(approximate='tanh')\n",
              "          (drop1): Dropout(p=0, inplace=False)\n",
              "          (norm): Identity()\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (drop2): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (t_adaLN_modulation): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=1024, out_features=6144, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer): FinalLayer(\n",
              "      (norm_final): LayerNorm((1024,), eps=1e-06, elementwise_affine=False)\n",
              "      (linear): Linear(in_features=1024, out_features=64, bias=True)\n",
              "      (adaLN_modulation): Sequential(\n",
              "        (0): SiLU()\n",
              "        (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
